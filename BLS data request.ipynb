{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f64363b8-fe1c-431f-9cb6-7ac9ee5ca3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLS code formats: https://www.bls.gov/help/hlpforma.htm#WP\n",
    "# PPI codes downloads: https://www.bls.gov/ppi/data-retrieval-guide/\n",
    "# PPI weights, seasonal adjustments: https://www.bls.gov/ppi/tables/commodity-special-requests.htm\n",
    "# manual data requests: https://data.bls.gov/series-report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef818a7c-011b-4b3b-ae89-0c94235f1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT DATA\n",
    "%reset -f\n",
    "import pandas as pd\n",
    "codes_backup= pd.read_csv(\"PPI_commodity_codes.csv\")\n",
    "\n",
    "codes_backup.columns=[\"identity\",\"content\"]\n",
    "\n",
    "\n",
    "#ID CONTENT:\n",
    "#wpu10-80: \"commodities/services\". Headline PPI based on these. 5th digit gives group, 6-7th gives item. Varied length for most disaggregated.\n",
    "#wpuIP: \"inputs to industry\", where industries are based on NAICS classification (like in other spreadsheet). Note that more complete (but experimental) version exists: https://www.bls.gov/ppi/input-indexes/\n",
    "#wpuFD: \"final demand\": goods grouped by the final consumer. \n",
    "#wpuID51-54: \"intermediate demand\": four stages of production (7 digits), with corresponding inputs (lowest most interesting).\n",
    "#wpuID59: intermediate demand total good/service/food/energy etc. inputs to each stage (all 10 digits).\n",
    "#wpu61-69: various inputs (materials, components, supplies, transportation) for different types of intermediate processing (manufacturing, construction, etc.)\n",
    "\n",
    "#wpd: discontinued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bdd0c05-4628-40b3-9ff8-7a503ea28a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN DATA, BASICS\n",
    "\n",
    "#Choose seasonally adjusted. ISSUE: only a few series are seasonally adjusted. BLS has test and most products in most years have too many other factors impacting them for obvious seasonal trends. \n",
    "#df['identity'] = df.identity.str.replace('wpu', 'wps')\n",
    "\n",
    "#PICK COMMODITIES/SERVICES.\n",
    "#select commodities/services indexes. Based on 4th character being a number between 1-15 or 30-80.\n",
    "codes_com = codes_backup[codes_backup[\"identity\"].str[3].str.isnumeric()]\n",
    "\n",
    "#select intermediate demand-final demand indexes. Based on 4th-5th characters being FD or ID.\n",
    "#codes_idfd = codes_backup[codes_backup[\"identity\"].str.startswith((\"wpuFD\", \"wpuID\"))]\n",
    "\n",
    "\n",
    "#ID CONTENT:\n",
    "#wpu10-80: \"commodities/services\". Headline PPI based on these(?). 5th digit gives group, 6-7th gives item. Varied length for most disaggregated.\n",
    "#wpuIP: \"inputs to industry\", where industries are based on NAICS classification (like in other spreadsheet). Note that more complete (but experimental) version exists: https://www.bls.gov/ppi/input-indexes/\n",
    "#wpuFD: \"final demand\": goods grouped by the final consumer. \n",
    "#wpuID51-54: \"intermediate demand\": four stages of production (7 digits), with corresponding inputs (lowest most interesting).\n",
    "#wpuID59: intermediate demand total good/service/food/energy etc. inputs to each stage (all 10 digits).\n",
    "#wpuID61-69: various inputs (materials, components, supplies, transportation) for different types of intermediate processing (manufacturing, construction, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8772c15e-42b6-491b-85c5-88f9377d2311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wq/93tz19ss0tb00nycxgx5d08h0000gn/T/ipykernel_4668/1556985213.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codes_com['code'] = codes_com.identity.str.extract('(\\d+)')\n",
      "/var/folders/wq/93tz19ss0tb00nycxgx5d08h0000gn/T/ipykernel_4668/1556985213.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codes_com['code_length'] = codes_com.code.str.len()\n"
     ]
    }
   ],
   "source": [
    "#DATA CLEANING: select which levels of aggregation to use.\n",
    "\n",
    "#create new column of just the numeric part of the codes\n",
    "codes_com['code'] = codes_com.identity.str.extract('(\\d+)')\n",
    "\n",
    "codes_com['code_length'] = codes_com.code.str.len()\n",
    "\n",
    "codes_com.groupby('code_length').size()\n",
    "# code_length\n",
    "# 2       85\n",
    "# 3      265\n",
    "# 4      429\n",
    "# 6      816\n",
    "# 7       28\n",
    "# 8     1420\n",
    "# 9      464\n",
    "# 10     110\n",
    "# 11      15\n",
    "\n",
    "#Even-digit codes (2,4,6,etc.) should add up to 100% of overall index and not overlap. In-between are special case indexes.\n",
    "#6-digit codes are 'baseline' on which other indexes (inc. FD-ID?) are based.\n",
    "#>6-digit codes some issues appear because they'll include products which are used at lots of stages of production/industries. Gets messy.\n",
    "#<6-digit are products which go into 6-digit codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c71a6-5ba4-4e7c-8b45-91e07f5d3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_com = codes_com[(codes_com['code_length']==2) | (codes_com['code_length']==6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c26b7d-86e5-4dd5-84e7-6be99052fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FUNCTION FOR MAKING API REQUEST\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def import_BLS_data(codes_list, start, end):\n",
    "\n",
    "    def chunker(data, length):  # Split list into chunks of equal size. For dealing with API max requests.\n",
    "        chunk_list = []  # Define list of chunks\n",
    "        for i in range(0, len(data), length):  # Iterate in steps.\n",
    "            chunk_list.append(data[i:i+length])  # Add each step as a separate list.\n",
    "        return chunk_list  # Return list of lists.\n",
    "\n",
    "    # API endpoint\n",
    "    url = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\n",
    "    series_id_list = list(codes_list[\"identity\"])\n",
    "    API_KEY = 'a54f3f2fc04f4c548e347be4e76f528f'\n",
    "    start_year = int(start)\n",
    "    end_year = int(end)\n",
    "    \n",
    "    # Iterate over the specified periods in chunks (10 years at a time)\n",
    "    for start_date in range(start_year, end_year, 10):\n",
    "        end_date = min(start_date + 9, end_year)\n",
    "        for chunk in chunker(series_id_list, 50):\n",
    "            chunk_list = list(chunk)\n",
    "            headers = {'Content-type': 'application/json'}\n",
    "            data = json.dumps({\n",
    "                \"seriesid\": chunk_list,\n",
    "                \"startyear\": str(start_date),\n",
    "                \"endyear\": str(end_date),\n",
    "                \"registrationkey\": API_KEY\n",
    "            })\n",
    "            response = requests.post(url, data=data, headers=headers)\n",
    "            json_data = json.loads(response.text)\n",
    "\n",
    "            # Loop through each series in the returned data\n",
    "            for series in json_data['Results']['series']:\n",
    "                series_id = series['seriesID']\n",
    "                series_points = series['data']\n",
    "\n",
    "                # Parse the dates and values for this series\n",
    "                for point in series_points:\n",
    "                    date_str = f\"{point['year']}-{point['period'][1:]}\"\n",
    "                    date = datetime.strptime(date_str, \"%Y-%m\").strftime('%Y-%m')\n",
    "                    value = float(point['value'])\n",
    "\n",
    "                    # Check if the column (date) already exists in the dataframe, otherwise create it\n",
    "                    if date not in codes_list.columns:\n",
    "                        codes_list[date] = None  # Add the date column\n",
    "\n",
    "                    # Find the index of the series_id in codes_list\n",
    "                    row_index = codes_list[codes_list['identity'] == series_id].index[0]\n",
    "\n",
    "                    # Assign the value directly to the corresponding row and column\n",
    "                    codes_list.at[row_index, date] = value\n",
    "\n",
    "    return codes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4adf37d9-274a-4892-b616-377ad35880c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE REQUEST AND BACKUP\n",
    "commodity_df = import_BLS_data(codes_com[['identity']], 2000, 2024)\n",
    "commodity_backup = commodity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527acaff-201f-4e30-9e98-1d2fff9b8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity_df.to_csv('PPI_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a215c53-e0eb-4c37-816c-ecc4e1db59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CLEAN DATA: create functions to select different levels of aggregation (industries, categories, lowest).\n",
    "# #function for selecting highest level (industries).\n",
    "\n",
    "# #function for selecting a level of disaggregation.\n",
    "# def one_level(df, column_name, level):\n",
    "#     df_industry = df.loc[df.identity.str.len()==int(level)]\n",
    "#     df_industry = df_industry.reset_index(drop=True)\n",
    "#     return df_industry\n",
    "\n",
    "# # #DROPPED\n",
    "# # #function for selecting an intermediate level of aggregation (categories or products).\n",
    "# # def intermediate_level(df, column_name, level):\n",
    "# #     df_intermediate = df.loc[df[column_name].str.len().isin([level,level+1])].reset_index(drop=True) #create dataframe where codes are of intermediate length (e.g. 5 or 6 for categories etc.) \n",
    "# #     rows_to_drop= set()\n",
    "\n",
    "# #     #sort to make sure related indexes are grouped.\n",
    "# #     df_intermediate= df_intermediate.sort_values(column_name)\n",
    "\n",
    "# #     #iterate over indexes. Collect indexes for which shorter version exists, so I can drop the less aggregated ones.\n",
    "# #     for i in range(len(df_intermediate)): \n",
    "# #         current_value = df_intermediate.iloc[i-1][column_name]\n",
    "# #         next_value = df_intermediate.iloc[i][column_name]\n",
    "        \n",
    "# #         # Check if the next row's string starts with the current row's string\n",
    "# #         while next_value.startswith(current_value):\n",
    "# #             rows_to_drop.add(i)\n",
    "# #             next_value += 1\n",
    "    \n",
    "# #     # Drop the rows from the DataFrame\n",
    "# #     df_intermediate = df_intermediate.drop(rows_to_drop).reset_index(drop=True)\n",
    "# #     return df_intermediate\n",
    "\n",
    "\n",
    "# #function for selecting two levels of disaggregation (when one level isn't available for all series).\n",
    "# def two_levels(df, column_name, level):\n",
    "#     df_intermediate= df.loc[df[column_name].str.len().isin([level])] #create dataframe with rows where index has level (e.g.6) characters\n",
    "#     higher_level = set(df[df[column_name].str.len().isin([level])][column_name].values) #create a set containing the six-digit indexes.\n",
    "    \n",
    "#     to_drop=set() #create empty set\n",
    "#     df_temp = df.loc[df[column_name].str.len().isin([level+1])] #create a dataframe with rows where index has level+1(e.g.7) characters.\n",
    "#     for i in range(len(df_temp)): #iterate over the rows in the level+1 dataframe.\n",
    "#         current_value = df_temp.iloc[i][column_name] \n",
    "        \n",
    "#         if current_value[:-1] in higher_level: #if current value is in the set of level indexes...\n",
    "#             to_drop.add(df_temp.index[i]) #add the row to the to_drop set.\n",
    "#     df_temp = df_temp.drop(to_drop).reset_index(drop=True) #drop rows from lower level dataframe.\n",
    "#     df_intermediate = pd.merge(df_intermediate, df_temp, how= \"outer\") #merge the higher and lower level dataframes\n",
    "#     df_intermediate = df_intermediate.sort_values(column_name)\n",
    "#     df_intermediate = df_intermediate.reset_index(drop=True)\n",
    "#     return df_intermediate\n",
    "\n",
    "   \n",
    "# #function for selecting most disaggregated level available.\n",
    "# def lowest_level(df, column_name):\n",
    "# # Function to drop rows where the next row consists of the same string plus extra characters\n",
    "#     df = df.sort_values(column_name)\n",
    "#     rows_to_drop = set()\n",
    "\n",
    "#     for i in range(len(df) - 1):\n",
    "#         current_value = df.iloc[i][column_name]\n",
    "#         next_value = df.iloc[i + 1][column_name]\n",
    "#         skipped= 0\n",
    "        \n",
    "#         # Check if the next row's string starts with the current row's string\n",
    "#         if next_value.startswith(current_value):\n",
    "#             rows_to_drop.add(df.index[i])\n",
    "            \n",
    "    \n",
    "#     # Drop the rows from the DataFrame\n",
    "#     df_cleaned = df.drop(rows_to_drop).reset_index(drop=True)\n",
    "    \n",
    "#     return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "438780ea-fdb1-46e9-b2c9-7dd03c241635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more readable version of intermediate_level filter written by Jacob. To learn from.\n",
    "# def intermediate_level_alt(df, column_name, level): \n",
    "#     df = df.sort_values(column_name)\n",
    "\n",
    "#     def filter(el): \n",
    "#         if len(el) == level: \n",
    "#             filter.level.add(el)\n",
    "#             return True\n",
    "#         elif len(el) == level + 1 and el[:-1] not in filter.level:\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "#     filter.level = set()\n",
    "#     res = df[df[column_name].apply(filter)]\n",
    "#     return res\n",
    "\n",
    "# df_test= filter_df(df_cose,\"identity\",6)\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5aabe565-2e04-47d6-94ea-71b92a872447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #DATA CLEANING: part datasets by level of aggregation\n",
    "\n",
    "# #for commodities/services, select industry, product group, and individual items.\n",
    "# df_cose_industry = one_level(df_cose, \"identity\", 5)\n",
    "# df_cose_group = two_levels(df_cose, \"identity\", 6)\n",
    "# df_cose_item = lowest_level(df_cose, \"identity\")\n",
    "# df_cose_item = df_cose_item[~df_cose_item.identity.str.contains(\"wps00000000\")] #take out all products index\n",
    "\n",
    "# #only the lowest level of aggregation is interesting for final demand imo.\n",
    "# df_fd= lowest_level(df_fd, \"identity\")\n",
    "\n",
    "# #for intermediate demand want the stages and each of their inputs.\n",
    "# df_id_stage = one_level(df_id, \"identity\", 7)\n",
    "# df_id_inputs = lowest_level(df_id, \"identity\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "a0684877-596f-4768-9dbe-15e330e88d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cose_data['Date'] = pd.to_datetime(cose_data['Date']).dt.date\n",
    "# cose_data = cose_data.set_index(['Date'])\n",
    "# cose_data = cose_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6488cfdd-e7cc-47f9-b9ed-838f68a84552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cose_data = cose_data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6289efc-9f42-416f-b9d6-76f4400f5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TO DO:\n",
    "#download weights and add them to df.\n",
    "    -download xlsx files\n",
    "    -create weights dataframe\n",
    "-create dataframe\n",
    "-add dates (dec of each year)\n",
    "-create columns for each code\n",
    "-import csv file\n",
    "-(possibly) delete top rows?\n",
    "-\n",
    "    -pull weights for index list\n",
    "\n",
    "\n",
    "#GOALS\n",
    "-how price categories have changed since 1900s.\n",
    "-\"map of the economy\"\n",
    "-volatility within goods categories and how this relates to headline inflation/category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d4e89-fb6f-4010-80f7-fa9d1a058f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
